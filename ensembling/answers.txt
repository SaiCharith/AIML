

Observations:

Bagging is relatively performintg better than boosting with same number of weak classifiers(evident from accuracy values and plots)
With increase in number of weak learners both the methods show an increase in test accuracy and saturate eventually.
With increase in trainging data size the accuracy on test data is increasing for both bagging and boosting which is expected
Size       500      1000     2000
Bagging  (73.1%)   (76.7%)  (79.3%)   
Boosting (74.3%)   (75.2%)  (77.7%)
With increase in ratio of number of samples to be used the accuracy of bagging increased, 
Ratio       0.5       1.0      1.5
Bagging   (75.8%)   (76.7%)  (79.3%)
Boosting  (75.2%)   (74.8%)  (75.3%)
With increase in number of iterations used to train perceptron the accuracy of both the ensembles increased  
iter         1         3        5
Bagging   (74.4%)   (76.7%)  (76.0%)
Boosting  (74.9%)   (75.2%)  (74.4%)

Number of base learners is probably incresing the test accuracy but the train accuracy is certainly improving
Bagging
#Learneres    Train   Val     Test      
10            89.4    76.5    75.9      
20            89.2    77.5    76.7      
30            89.2    77.5    76.2      
40            89.6    77.6    76.8      
50            89.7    78.0    76.7  
Boosting    
10            88.9    76.7    74.3
20            90.7    77.4    75.2
30            91.4    78.0    74.3
40            91.8    77.3    75.1
50            92.2    77.5    76.5


Answers:

1)It is evident from the plots that the training accuracy of boosting is better than bagging.
Further the training accuracy of boosting is increasing while that of bagging is saturated.
This is expected as there are theoritical results on training accuracy of boosting, which say that with more than certain number of weak learners, the training accuracy is more than a certain value. This is equivalent to saying that we can achieve any amount of accuracy on train data provided we use sufficient number of base learners.
There are no such gaurentees with bagging.  

2) If the statement "An ensemble combining perceptrons with weighted majority cannot be represented as an equivalent single perceptron" is not true then,

Consider a senario in 2D where we have
			Points Theta(degrees)
			  +     0 to 45
			  -		-45 to 0
			  +		45 to 90
			  - 	-90 to -45

Clearly this can be classified correctly by an ensamble of the perceptrons (-1,1),(0,1),(1,1).
But a perceptron cannot which contrdicts our assumption, hence the statement is true.      